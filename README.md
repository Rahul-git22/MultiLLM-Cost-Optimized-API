# Multi-Provider Language Model API

This project is a FastAPI application designed to route user requests to multiple language model providers, optimizing for cost and reliability. It includes features such as request logging, fallback logic, and a standardized API endpoint for easy integration.

## Prerequisites

To run this project, you will need Python 3.7+ installed on your system. Additionally, you will need to install the required packages specified in the `requirements.txt` file.

## Setup

1.  Clone the repository or download the project files.
2.  Navigate to the project directory.
3.  Install the required packages:

  ```bash
    
    pip install -r requirements.txt
  ```

## Configuration

Before running the application, configure the `providers.yaml` file with the necessary details for each model provider, including the API keys, endpoints, costs, models, and priorities.

**Example `providers.yaml`:**

```yaml
providers:
  - name: groq
    endpoint: "https://api.groq.com/openai/v1/chat/completions"
    priority: 2
    cost_per_1k_tokens: 0.002
    api_key: "--------------------------------------------"
    model: "deepseek-r1-distill-qwen-32b"
  - name: groq
    endpoint: "https://api.groq.com/openai/v1/chat/completions"
    priority: 1
    cost_per_1k_tokens: 0.002
    api_key: "-------------------------------------------------"
    model: "llama-3.1-8b-instant"
```
## Explanation of Configuration:

- name: A user-friendly name for the provider.
- endpoint: The API endpoint for the provider.
- priority: The priority of the provider (lower numbers have higher priority).
- cost_per_1k_tokens: The cost per 1000 tokens for the provider.
- api_key: The API key for authenticating with the provider.
- model: The model to use for the provider.

## Usage
To start the FastAPI server, run the following command:

```Bash

uvicorn main:app --reload
```
This command will start the server in development mode with hot reloading enabled.

## API Endpoints
The application provides a single endpoint for generating responses based on user prompts.

Generate Response
- Endpoint: /generate

- Method: POST

Request Body:

```JSON

{
  "prompt": "Your text prompt here"
}
```
Response:

```JSON

{
  "modelUsed": "mixtral-8x7b-32768",
  "tokens": 50,
  "cost": 0.0001,
  "response": "Quantum computing is a type of computing that uses quantum bits or qubits to process information..."
}
```
Example Request using Python
You can use Python with the requests library to send a request:

```Python

import requests
import json

url = "[http://127.0.0.1:8000/generate](http://127.0.0.1:8000/generate)"
headers = {"Content-Type": "application/json"}
data = {"prompt": "Explain quantum computing"}

response = requests.post(url, headers=headers, json=json.dumps(data))
print(response.json())
```
## Logging
To review the logs generated by the application, check the app.log file:

```Bash

cat app.log
```
This file will contain entries with usage data, including the model used, tokens, cost, time taken, and success status.
